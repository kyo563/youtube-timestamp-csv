import streamlit as st
import re
import csv
import io
import requests
import urllib.parse
from datetime import datetime
from typing import Tuple, List, Optional, Dict
from zoneinfo import ZoneInfo
import unicodedata
import pandas as pd

# ==============================
# åŸºæœ¬è¨­å®š
# ==============================
st.set_page_config(page_title="ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—CSVå‡ºåŠ›", layout="centered")

st.title("YouTube CSVãƒ„ãƒ¼ãƒ«")
st.write("ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—CSVç”Ÿæˆã¨ã‚·ãƒ§ãƒ¼ãƒˆå‹•ç”»CSVç”Ÿæˆ")

# è¡¨ç¤ºåã®åŒºåˆ‡ã‚Šï¼ˆä¾‹: 20250101 My Video Titleï¼‰
DATE_TITLE_SEPARATOR = " "
# ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ã¯å›ºå®š
TZ_NAME = "Asia/Tokyo"

# å…±é€šAPIã‚­ãƒ¼ï¼ˆSecretså„ªå…ˆï¼‰
GLOBAL_API_KEY = st.secrets.get("YT_API_KEY", "")

# ==============================
# å…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# ==============================
def resolve_api_key(
    default_key: str,
    input_state_key: str,
    expander_label: str,
    input_label: str = "YT_API_KEY",
) -> str:
    """
    Secrets ã«è¨­å®šã•ã‚ŒãŸ APIã‚­ãƒ¼ã‚’å„ªå…ˆã—ã€ç„¡ã„å ´åˆã ã‘ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰å…¥åŠ›æ¬„ã‚’è¡¨ç¤ºã—ã¦å–å¾—ã—ã¾ã™ã€‚
    """
    api_key = default_key
    if not api_key:
        with st.expander(expander_label):
            api_key = st.text_input(input_label, type="password", key=input_state_key)
    return api_key or ""

def to_csv(rows: List[List[str]]) -> str:
    buf = io.StringIO()
    csv.writer(buf, quoting=csv.QUOTE_ALL).writerows(rows)
    return buf.getvalue()

def make_excel_hyperlink(url_: str, label: str) -> str:
    """Excelç”¨ HYPERLINK é–¢æ•°æ–‡å­—åˆ—."""
    safe = (label or "").replace('"', '""')
    return f'=HYPERLINK("{url_}","{safe}")'

def is_valid_youtube_url(u: str) -> bool:
    return bool(re.match(r"^(https?:\/\/)?(www\.)?(youtube\.com|youtu\.?be)\/.+$", u or ""))

def normalize_text(s: str) -> str:
    """å…¨è§’â†’åŠè§’ãªã©è»½å¾®ãªæ­£è¦åŒ–ã¨ç©ºç™½æ•´å½¢ã§ã™ã€‚â€»ä¼¸ã°ã—æ£’ã€Œãƒ¼ã€ã¯å¤‰æ›ã—ã¾ã›ã‚“ã€‚"""
    s = (s or "").replace("ï¼", "/")   # å…¨è§’ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã®ã¿åŠè§’ã¸
    s = s.replace("ã€€", " ").strip()  # å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹â†’åŠè§’
    return re.sub(r"\s+", " ", s)     # é€£ç¶šç©ºç™½ã‚’1ã¤ã«

def extract_video_id(u: str) -> Optional[str]:
    """URLã‹ã‚‰Video IDã‚’æŠ½å‡ºï¼ˆwatch?v= / youtu.be / shorts/ ã«å¯¾å¿œï¼‰ã§ã™ã€‚"""
    if not u:
        return None
    try:
        pr = urllib.parse.urlparse(u)
        host = (pr.netloc or "").lower()
        path = pr.path or ""
        qs = urllib.parse.parse_qs(pr.query or "")
        if "youtu.be" in host:
            seg = path.strip("/").split("/")
            return seg[0] if seg and seg[0] else None
        if "youtube.com" in host:
            if "v" in qs and qs["v"]:
                return qs["v"][0]
            if path.startswith("/shorts/"):
                after = path.split("/shorts/", 1)[1]
                return after.split("/")[0].split("?")[0]
        return None
    except Exception:
        return None

def normalize_manual_date_input(raw: str, tz_name: str) -> Optional[str]:
    """
    æ‰‹å‹•å…¥åŠ›ã•ã‚ŒãŸæ—¥ä»˜æ–‡å­—åˆ—ã‚’ yyyymmdd ã«æ­£è¦åŒ–ã—ã¦è¿”ã—ã¾ã™ã€‚
    """
    s = (raw or "").strip()
    if not s:
        return None

    # å…¨è§’â†’åŠè§’ï¼ˆæ•°å­—ãƒ»ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ãªã©ï¼‰
    s = unicodedata.normalize("NFKC", s)

    # æ—¥æœ¬èªã®å¹´/æœˆ/æ—¥ã‚’ / ã«çµ±ä¸€
    s = s.replace("å¹´", "/").replace("æœˆ", "/").replace("æ—¥", "")

    # ., - ã¨ç©ºç™½ã‚’ / ã«çµ±ä¸€
    s = re.sub(r"[.\-]", "/", s)
    s = re.sub(r"\s+", "/", s)
    s = s.strip("/")

    # ãƒ‘ã‚¿ãƒ¼ãƒ³1: ã™ã§ã«8æ¡æ•°å­—ï¼ˆyyyymmddï¼‰
    if re.fullmatch(r"\d{8}", s):
        y, m, d = int(s[0:4]), int(s[4:6]), int(s[6:8])
    else:
        parts = s.split("/")
        if len(parts) == 3:
            # 2025/3/20 ãªã©
            try:
                y, m, d = map(int, parts)
            except ValueError:
                return None
        elif len(parts) == 2:
            # 11/19, 3/20 ãªã© â†’ å¹´ã¯ç¾åœ¨å¹´
            today = datetime.now(ZoneInfo(tz_name)).date()
            y = today.year
            try:
                m, d = map(int, parts)
            except ValueError:
                return None
        else:
            return None

    # 2æ¡å¹´ãŒæ¥ãŸå ´åˆã¯ 2000å¹´ä»£ã¨ã—ã¦æ‰±ã†
    if y < 100:
        y += 2000

    try:
        dt = datetime(y, m, d)
    except ValueError:
        # å­˜åœ¨ã—ãªã„æ—¥ä»˜ãªã‚‰ None
        return None

    return dt.strftime("%Y%m%d")

@st.cache_data(show_spinner=False, ttl=3600)
def fetch_video_title_from_oembed(watch_url: str) -> str:
    """oEmbedã§å‹•ç”»ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—ï¼ˆAPIã‚­ãƒ¼ä¸è¦ï¼‰ã€‚å¤±æ•—æ™‚ã¯æ—¢å®šåã§ã™ã€‚"""
    try:
        r = requests.get("https://www.youtube.com/oembed", params={"url": watch_url, "format": "json"}, timeout=6)
        if r.status_code == 200:
            title = (r.json().get("title") or "").strip()
            return title if title else "YouTubeå‹•ç”»"
    except Exception:
        pass
    return "YouTubeå‹•ç”»"

def iso_utc_to_tz_yyyymmdd(iso_str: str, tz_name: str) -> Optional[str]:
    """
    ISO8601(UTC, 'Z' ã¾ãŸã¯ 'Z+å°æ•°') ã‚’ tz_name ã¸å¤‰æ›ã— yyyymmdd ã‚’è¿”ã—ã¾ã™ã€‚
    YouTube publishedAt / actualStartTime / scheduledStartTime å…±é€šåˆ©ç”¨ã€‚
    """
    if not iso_str:
        return None
    try:
        s = iso_str
        if s.endswith("Z"):
            s = s[:-1] + "+00:00"
        dt_utc = datetime.fromisoformat(s)  # å°æ•°ç§’ä»˜ãã‚‚å¯¾å¿œ
        dt_local = dt_utc.astimezone(ZoneInfo(tz_name))
        return dt_local.strftime("%Y%m%d")
    except Exception:
        return None

# ==============================
# ã‚¿ãƒ–1ï¼šã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—CSVã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼ç”¨é–¢æ•°
# ==============================
def parse_line(line: str, flip: bool) -> Tuple[Optional[int], Optional[str], Optional[str]]:
    """
    å…ˆé ­ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’èª­ã¿å–ã‚Šã€(seconds, artist, song) ã‚’è¿”ã—ã¾ã™ã€‚
    """
    m = re.match(r"^(\d{1,2}:)?(\d{1,2}):(\d{2})", line)
    if not m:
        return (None, None, None)
    time_str = m.group(0)
    parts = list(map(int, time_str.split(":")))
    if len(parts) == 3:
        seconds = parts[0] * 3600 + parts[1] * 60 + parts[2]
    else:
        seconds = parts[0] * 60 + parts[1]
    info = line[len(time_str):].strip()

    # åŒºåˆ‡ã‚Šï¼ˆãƒ¼ã¯é™¤å¤–ï¼‰ã€‚å¯¾è±¡: -, â€”, â€“, â€•, ï¼, /, ï¼, by, BY
    msep = re.search(r"\s(-|â€”|â€“|â€•|ï¼|/|ï¼|by|BY)\s", info)
    if msep:
        left  = normalize_text(info[:msep.start()].strip())
        right = normalize_text(info[msep.end():].strip())
        if not flip:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼šå³â†’å·¦ï¼ˆå³=ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆã€å·¦=æ›²åï¼‰
            artist, song = right or "N/A", left or "N/A"
        else:
            # åè»¢ï¼šå·¦â†’å³ï¼ˆå·¦=ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆã€å³=æ›²åï¼‰
            artist, song = left or "N/A", right or "N/A"
        return (seconds, artist, song)

    # åŒºåˆ‡ã‚ŠãŒãªã„å ´åˆï¼šå…¨æ–‡ã‚’æ›²åæ‰±ã„
    return (seconds, "N/A", normalize_text(info) or "N/A")

@st.cache_data(show_spinner=False, ttl=3600)
def fetch_best_display_date_and_sources(video_id: str, api_key: str, tz_name: str) -> Dict[str, Optional[str]]:
    """
    videos?part=snippet,liveStreamingDetails ã‚’å–å¾—ã€‚
    å„ªå…ˆé †ä½: actualStartTime â†’ scheduledStartTime â†’ publishedAtã€‚
    """
    result: Dict[str, Optional[str]] = {
        "chosen_yyyymmdd": None,
        "source": None,
    }
    if not api_key:
        return result
    try:
        url = "https://www.googleapis.com/youtube/v3/videos"
        params = {"part": "snippet,liveStreamingDetails", "id": video_id, "key": api_key}
        r = requests.get(url, params=params, timeout=6)
        if r.status_code != 200:
            return result
        items = (r.json() or {}).get("items", [])
        if not items:
            return result

        item = items[0]
        snippet = item.get("snippet", {}) or {}
        live = item.get("liveStreamingDetails", {}) or {}

        publishedAt = snippet.get("publishedAt")
        actualStartTime = live.get("actualStartTime")
        scheduledStartTime = live.get("scheduledStartTime")

        publishedAt_local = iso_utc_to_tz_yyyymmdd(publishedAt, tz_name) if publishedAt else None
        actualStartTime_local = iso_utc_to_tz_yyyymmdd(actualStartTime, tz_name) if actualStartTime else None
        scheduledStartTime_local = iso_utc_to_tz_yyyymmdd(scheduledStartTime, tz_name) if scheduledStartTime else None

        if actualStartTime_local:
            result["chosen_yyyymmdd"] = actualStartTime_local
            result["source"] = "actualStartTime"
        elif scheduledStartTime_local:
            result["chosen_yyyymmdd"] = scheduledStartTime_local
            result["source"] = "scheduledStartTime"
        elif publishedAt_local:
            result["chosen_yyyymmdd"] = publishedAt_local
            result["source"] = "publishedAt"

        return result
    except Exception:
        return result

def generate_rows(
    u: str,
    timestamps_text: str,
    tz_name: str,
    api_key: str,
    manual_yyyymmdd: str,
    flip: bool
) -> Tuple[List[List[str]], List[dict], List[str], str]:
    """å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã‚’è§£æã—ã€CSVè¡Œãƒ»ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼è¡Œãƒ»æœªè§£æè¡Œãƒ»å‹•ç”»ã‚¿ã‚¤ãƒˆãƒ«ã‚’è¿”ã—ã¾ã™ã€‚"""
    vid = extract_video_id(u)
    if not vid:
        raise ValueError("URLã‹ã‚‰ãƒ“ãƒ‡ã‚ªIDã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
    base_watch = f"https://www.youtube.com/watch?v={vid}"

    # ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆoEmbedï¼‰
    video_title = fetch_video_title_from_oembed(base_watch)

    # æ—¥ä»˜ï¼ˆãƒ©ã‚¤ãƒ–/ãƒ—ãƒ¬ãƒŸã‚¢å„ªå…ˆ + ãƒ­ãƒ¼ã‚«ãƒ«TZå¤‰æ›ï¼‰
    date_info: Dict[str, Optional[str]] = {"chosen_yyyymmdd": None, "source": None}
    if api_key:
        date_info = fetch_best_display_date_and_sources(vid, api_key, tz_name)

    date_yyyymmdd: Optional[str] = date_info.get("chosen_yyyymmdd")
    date_source: Optional[str] = date_info.get("source")

    # APIã§å–å¾—ä¸å¯ãƒ»æœªè¨­å®šæ™‚ã¯æ‰‹å‹•æ—¥ä»˜
    if not date_yyyymmdd and manual_yyyymmdd and re.fullmatch(r"\d{8}", manual_yyyymmdd):
        date_yyyymmdd = manual_yyyymmdd
        date_source = "manual"

    display_name = f"{date_yyyymmdd}{DATE_TITLE_SEPARATOR}{video_title}" if date_yyyymmdd else video_title

    # ãƒ˜ãƒƒãƒ€ã¯3åˆ—å›ºå®š
    rows: List[List[str]] = [["ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆå", "æ¥½æ›²å", "YouTubeãƒªãƒ³ã‚¯"]]
    parsed_preview: List[dict] = []
    invalid_lines: List[str] = []

    for raw in (timestamps_text or "").splitlines():
        line = normalize_text(raw)
        if not line:
            continue
        sec, artist, song = parse_line(line, flip)
        if sec is None:
            invalid_lines.append(raw)
            continue

        jump = f"{base_watch}&t={sec}s"
        hyperlink = make_excel_hyperlink(jump, display_name)
        rows.append([artist, song, hyperlink])
        parsed_preview.append({
            "time_seconds": sec,
            "artist": artist,
            "song": song,
            "display_name": display_name,
            "date_source": date_source,
            "hyperlink_formula": hyperlink,
        })

    if len(rows) == 1:
        raise ValueError("æœ‰åŠ¹ãªã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ãã®æ¥½æ›²ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

    return rows, parsed_preview, invalid_lines, video_title

# ==============================
# ã‚¿ãƒ–2ï¼šShorts â†’ CSV ç”¨é–¢æ•°
# ==============================
def extract_channel_id_from_url(url: str, api_key: str) -> Optional[str]:
    """
    /channel/UCxxxx â†’ ãã®ã¾ã¾è¿”ã™ã€‚
    /@handle ã‚„ /c/xxxx â†’ search.list(type=channel) ã§è§£æ±ºï¼ˆAPIã‚­ãƒ¼å¿…é ˆï¼‰ã€‚
    """
    try:
        pr = urllib.parse.urlparse(url)
        path = pr.path or ""
        # /channel/UCxxxx
        m = re.search(r"/channel/(UC[\w-]+)", path)
        if m:
            return m.group(1)
        # /@handle
        m = re.search(r"/@([^/?#]+)", path)
        if m and api_key:
            handle = m.group(1)
            resp = requests.get(
                "https://www.googleapis.com/youtube/v3/search",
                params={"part": "snippet", "type": "channel", "q": handle, "maxResults": 5, "key": api_key},
                timeout=8,
            ).json()
            for it in resp.get("items", []):
                ch_id = it.get("id", {}).get("channelId")
                if ch_id:
                    return ch_id
        # /c/ ã‚„ /user/ ã®ã‚±ãƒ¼ã‚¹ã‚‚æ¤œç´¢ã§å¯¾å¿œï¼ˆAPIå‰æï¼‰
        if api_key:
            candidate = [p for p in path.split("/") if p][-1]
            if candidate:
                resp = requests.get(
                    "https://www.googleapis.com/youtube/v3/search",
                    params={"part": "snippet", "type": "channel", "q": candidate, "maxResults": 5, "key": api_key},
                    timeout=8,
                ).json()
                for it in resp.get("items", []):
                    ch_id = it.get("id", {}).get("channelId")
                    if ch_id:
                        return ch_id
        return None
    except Exception:
        return None

def list_channel_videos(channel_id: str, api_key: str, limit: int = 50) -> List[str]:
    """
    search.list ã§ãƒãƒ£ãƒ³ãƒãƒ«å†…å‹•ç”»ã® videoId ã‚’æ–°ç€é †ã§å–å¾—ã—ã¾ã™ï¼ˆAPIã‚­ãƒ¼å¿…é ˆï¼‰ã€‚
    """
    ids: List[str] = []
    token = None
    while len(ids) < limit:
        params = {
            "part": "id", "type": "video", "channelId": channel_id,
            "maxResults": 50, "order": "date", "key": api_key
        }
        if token:
            params["pageToken"] = token
        data = requests.get("https://www.googleapis.com/youtube/v3/search", params=params, timeout=8).json()
        for it in data.get("items", []):
            vid = it.get("id", {}).get("videoId")
            if vid:
                ids.append(vid)
        token = data.get("nextPageToken")
        if not token:
            break
    return ids[:limit]

def iso8601_to_seconds(iso: str) -> int:
    m = re.match(r"PT(?:(\d+)H)?(?:(\d+)M)?(?:(\d+)S)?", iso or "")
    h = int(m.group(1) or 0) if m else 0
    m_ = int(m.group(2) or 0) if m else 0
    s = int(m.group(3) or 0) if m else 0
    return h*3600 + m_*60 + s

def fetch_video_meta(video_ids: List[str], api_key: str):
    """
    videos.list ã§ title / duration / publishedAt ã‚’å–å¾—ã—ã¾ã™ã€‚
    è¿”å´: [{'videoId', 'title', 'seconds', 'yyyymmdd'}, ...]
    """
    out = []
    for i in range(0, len(video_ids), 50):
        chunk = ",".join(video_ids[i:i+50])
        data = requests.get(
            "https://www.googleapis.com/youtube/v3/videos",
            params={"part": "snippet,contentDetails", "id": chunk, "key": api_key},
            timeout=8,
        ).json()
        for it in data.get("items", []):
            vid = it.get("id")
            snip = it.get("snippet", {}) or {}
            cdet = it.get("contentDetails", {}) or {}
            title = (snip.get("title") or "").strip()
            dur = iso8601_to_seconds(cdet.get("duration"))
            ymd = iso_utc_to_tz_yyyymmdd(snip.get("publishedAt", ""), TZ_NAME)
            out.append({"videoId": vid, "title": title, "seconds": dur, "yyyymmdd": ymd})
    return out

def clean_for_parse(s: str) -> str:
    # ä¼¸ã°ã—æ£’ã€Œãƒ¼ã€ã¯ä¸€åˆ‡è§¦ã‚‰ãªã„ã€‚ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ã‚„URLã¯é™¤å»ã€‚
    s = (s or "").replace("ï¼", "/")
    s = re.sub(r"https?://\S+", " ", s)
    s = re.sub(r"#\S+", " ", s)
    s = re.sub(r"[ã€\[][^ã€‘\]]*[ã€‘\]]", " ", s)  # ã€ã€‘ã‚„[]ã®ãƒ¡ã‚¿è¡¨è¨˜ã‚’é™¤å»
    s = re.sub(r"\s+", " ", s).strip()
    return s

def split_artist_song_from_title(title: str) -> Tuple[str, str]:
    """
    ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰ (artist, song) ã‚’æ¨å®šã—ã¦è¿”ã—ã¾ã™ã€‚
    """
    t = clean_for_parse(title)

    # 1) å¼•ç”¨å†…ãŒæ›²åãƒ‘ã‚¿ãƒ¼ãƒ³
    q = re.search(r'[ã€Œã€â€œ"](.+?)[ã€ã€â€"]', t)
    if q:
        song = q.group(1).strip()
        artist = (t[:q.start()] + t[q.end():]).strip(" -/byBY")
        artist = re.sub(r"\s+", " ", artist).strip()
        return artist if artist else "N/A", song if song else "N/A"

    # 2) å‰å¾Œã«ç©ºç™½ã®ã‚ã‚‹æ˜ç¤ºåŒºåˆ‡ã‚Šï¼ˆãƒ¼ã¯åŒºåˆ‡ã‚Šæ‰±ã„ã—ãªã„ï¼‰
    m = re.search(r"\s(-|â€”|â€“|â€•|ï¼|/|ï¼|by|BY)\s", t)
    if m:
        left = t[:m.start()].strip()
        right = t[m.end():].strip()
        # è‹±å­—å¤šã„æ–¹ã‚’ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆï¼ˆç°¡æ˜“ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯ï¼‰
        alpha_left = len(re.findall(r"[A-Za-z]", left))
        alpha_right = len(re.findall(r"[A-Za-z]", right))
        artist, song = (left, right) if alpha_left > alpha_right else (right, left)
        return artist or "N/A", song or "N/A"

    # 3) ç©ºç™½ç„¡ã—ã® "/" åŒºåˆ‡ã‚Šï¼ˆä¾‹: "æ›²å/ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆ"ï¼‰
    if "/" in t:
        if t.count("/") == 1 and not t.startswith("/") and not t.endswith("/"):
            left, right = [part.strip() for part in t.split("/", 1)]
            if left and right:
                alpha_left = len(re.findall(r"[A-Za-z]", left))
                alpha_right = len(re.findall(r"[A-Za-z]", right))
                artist, song = (left, right) if alpha_left > alpha_right else (right, left)
                return artist or "N/A", song or "N/A"

    # 4) æ±ç”¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆå…¨éƒ¨æ›²åæ‰±ã„ï¼‰
    return "N/A", t or "N/A"

# --------- éå…¬å¼ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆAPIã‚­ãƒ¼ç„¡ã—æ™‚ã®ç°¡æ˜“æŠ½å‡ºï¼‰ ----------
def scrape_shorts_ids_from_web(url: str, limit: int = 50) -> List[str]:
    """
    /@handle/shorts ãªã©ã®HTMLã‹ã‚‰ "videoId":"XXXX" ã‚’æ‹¾ã†ãƒ™ã‚¹ãƒˆã‚¨ãƒ•ã‚©ãƒ¼ãƒˆã€‚
    """
    try:
        pr = urllib.parse.urlparse(url)
        base = f"{pr.scheme}://{pr.netloc}"
        m = re.search(r"/@[^/?#]+", pr.path)
        if m:
            target = base + m.group(0) + "/shorts"
        else:
            target = base + pr.path.rstrip("/") + "/shorts"
        html = requests.get(target, timeout=8, headers={"User-Agent": "Mozilla/5.0"}).text
        vids = re.findall(r'"videoId":"([a-zA-Z0-9_-]{11})"', html)
        seen = set()
        uniq = []
        for v in vids:
            if v not in seen:
                seen.add(v)
                uniq.append(v)
            if len(uniq) >= limit:
                break
        return uniq
    except Exception:
        return []

# ==============================
# ã‚¿ãƒ–ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ
# ==============================
tab1, tab2 = st.tabs(["â± ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—CSV", "ğŸ¬ Shortsâ†’CSV"])

# ---------------- ã‚¿ãƒ–1ï¼šã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—CSVã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼ ----------------
with tab1:
    st.subheader("ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—CSVã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼")
    st.write("YouTubeå‹•ç”»ã®URLã¨ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãƒªã‚¹ãƒˆã‹ã‚‰CSVã‚’ç”Ÿæˆã—ã¾ã™ã€‚")

    url = st.text_input(
        "1. YouTubeå‹•ç”»ã®URL",
        placeholder="https://www.youtube.com/watch?v=xxxxxxxxxxx",
        key="ts_url",
    )

    api_key_ts = resolve_api_key(
        default_key=GLOBAL_API_KEY,
        input_state_key="ts_api_key",
        expander_label="YouTube APIã‚­ãƒ¼ï¼ˆä»»æ„ã€‚æœªè¨­å®šã§ã‚‚æ‰‹å‹•ã§å…¬é–‹æ—¥ã‚’æŒ‡å®šã§ãã¾ã™ï¼‰",
    )

    manual_date_raw_ts: str = ""
    manual_date_ts: str = ""

    if not api_key_ts:
        manual_date_raw_ts = st.text_input(
            "å…¬é–‹æ—¥ã‚’æ‰‹å‹•æŒ‡å®šï¼ˆAPIæœªè¨­å®šæ™‚ã«åˆ©ç”¨ï¼ä»»æ„ï¼‰",
            placeholder="ä¾‹: 2025/11/19, 11/19, 3æœˆ20æ—¥ ãªã©",
            key="ts_manual_date_raw",
        )

    timestamps_input_ts = st.text_area(
        "2. æ¥½æ›²ãƒªã‚¹ãƒˆï¼ˆã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ãï¼‰",
        placeholder="ä¾‹ï¼š\n0:35 æ›²åA / ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆåA\n6:23 æ›²åB - ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆåB\n1:10:05 æ›²åC by ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆåC",
        height=220,
        key="timestamps_input_ts",
    )

    # æ‰‹å‹•æ—¥ä»˜å…¥åŠ›ã®æ­£è¦åŒ–
    if not api_key_ts and manual_date_raw_ts:
        normalized = normalize_manual_date_input(manual_date_raw_ts, TZ_NAME)
        if normalized:
            manual_date_ts = normalized
            st.caption(f"è§£é‡ˆã•ã‚ŒãŸå…¬é–‹æ—¥: {manual_date_ts}")
        else:
            manual_date_ts = ""
            st.error("æ—¥ä»˜ã¨ã—ã¦è§£é‡ˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ä¾‹: 2025/11/19, 11/19, 3æœˆ20æ—¥ ãªã©ã®å½¢å¼ã§å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")

    c1, c2 = st.columns(2)
    with c1:
        st.toggle("å·¦å³åè»¢", value=False, key="flip_ts")
        preview_clicked = st.button("ğŸ” ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼è¡¨ç¤º", key="preview_ts")
    with c2:
        csv_clicked = st.button("ğŸ“¥ CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆ", key="csv_ts")

    # ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ç”Ÿæˆ
    if preview_clicked:
        timestamps_text = st.session_state.get("timestamps_input_ts", "")
        flip = st.session_state.get("flip_ts", False)

        if not url or not timestamps_text:
            st.error("URLã¨æ¥½æ›²ãƒªã‚¹ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        elif not is_valid_youtube_url(url):
            st.error("æœ‰åŠ¹ãªYouTube URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        else:
            try:
                rows, preview, invalid, video_title = generate_rows(
                    url, timestamps_text, TZ_NAME, api_key_ts, manual_date_ts, flip
                )
                st.session_state["ts_preview_df"] = preview
                st.session_state["ts_preview_invalid"] = invalid
                st.session_state["ts_preview_title"] = video_title
                st.success(f"è§£ææˆåŠŸï¼š{len(preview)}ä»¶ã€‚æœªè§£æï¼š{len(invalid)}ä»¶ã€‚ä¸‹éƒ¨ã«ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’è¡¨ç¤ºã—ã¾ã—ãŸã€‚")
            except Exception as e:
                st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")

    # CSVç”Ÿæˆ
    if csv_clicked:
        timestamps_text = st.session_state.get("timestamps_input_ts", "")
        flip = st.session_state.get("flip_ts", False)
        if not url or not timestamps_text:
            st.error("URLã¨æ¥½æ›²ãƒªã‚¹ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        elif not is_valid_youtube_url(url):
            st.error("æœ‰åŠ¹ãªYouTube URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        else:
            try:
                rows, preview, invalid, video_title = generate_rows(
                    url, timestamps_text, TZ_NAME, api_key_ts, manual_date_ts, flip
                )
                csv_content = to_csv(rows)

                # ãƒ•ã‚¡ã‚¤ãƒ«åã‚µãƒ‹ã‚¿ã‚¤ã‚ºï¼ˆå…±é€šé–¢æ•°ã«ã—ã¦ã‚‚OKã§ã™ãŒã“ã“ã ã‘ãªã®ã§ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³ï¼‰
                download_name = re.sub(r'[\\/:*?"<>|\x00-\x1F]', "_", video_title or "").strip().strip(".") or "youtube_song_list"
                if len(download_name) > 100:
                    download_name = download_name[:100]
                download_name += ".csv"

                st.success("CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆã—ã¾ã—ãŸã€‚ä¸‹ã®ãƒœã‚¿ãƒ³ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™ã€‚")
                st.download_button(
                    label="CSVã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                    data=csv_content.encode("utf-8-sig"),
                    file_name=download_name,
                    mime="text/csv"
                )
                if invalid:
                    st.info(f"æœªè§£æè¡Œï¼š{len(invalid)}ä»¶ã€‚å…¥åŠ›ã®æ›¸å¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            except Exception as e:
                st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")

    # ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼è¡¨ç¤º
    if "ts_preview_df" in st.session_state:
        st.subheader("ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼")

        df = pd.DataFrame(st.session_state["ts_preview_df"])

        st.dataframe(
            df,
            use_container_width=True,
            column_config={
                "time_seconds": st.column_config.NumberColumn("ç§’æ•°", width="small"),
                "artist": st.column_config.TextColumn("ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆå", width="medium"),
                "song": st.column_config.TextColumn("æ¥½æ›²å", width="large"),
                "display_name": st.column_config.TextColumn("ãƒªãƒ³ã‚¯è¡¨ç¤ºå", width="large"),
                "date_source": st.column_config.TextColumn("æ—¥ä»˜ã‚½ãƒ¼ã‚¹", width="small"),
                "hyperlink_formula": st.column_config.TextColumn("Excelç”¨ãƒªãƒ³ã‚¯å¼", width="large"),
            },
        )

        st.caption(f"å‹•ç”»ã‚¿ã‚¤ãƒˆãƒ«ï¼š{st.session_state.get('ts_preview_title', '')}")

        invalid_lines = st.session_state.get("ts_preview_invalid", [])
        if invalid_lines:
            with st.expander("æœªè§£æè¡Œã®ä¸€è¦§"):
                st.code("\n".join(invalid_lines))

    with st.expander("ğŸ‘€ ã‚µãƒ³ãƒ—ãƒ«å…¥åŠ›ã®ãƒ’ãƒ³ãƒˆ"):
        st.markdown("- URLä¾‹: `https://www.youtube.com/watch?v=dQw4w9WgXcQ`")
        st.markdown("- è¡Œæ›¸å¼: `MM:SS` ã¾ãŸã¯ `HH:MM:SS` + åŠè§’ã‚¹ãƒšãƒ¼ã‚¹ + ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆåŒºåˆ‡ã‚Š ` - `, ` / `, ` by ` ãªã©ï¼‰")

# ---------------- ã‚¿ãƒ–2ï¼šShorts â†’ CSVï¼ˆæ›²åãƒ»ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆæ¨å®šï¼‰ ----------------
with tab2:
    st.subheader("ã‚·ãƒ§ãƒ¼ãƒˆ â†’ CSV")
    st.write(
        "ãƒãƒ£ãƒ³ãƒãƒ«URLã‹ã‚‰ã‚·ãƒ§ãƒ¼ãƒˆå‹•ç”»ã‚’å–å¾—ã—ã€ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰ **æ¥½æ›²å/ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆå** ã‚’æ¨å®šã—ã¦ "
        "CSVï¼ˆã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆå, æ¥½æ›²å, ã‚·ãƒ§ãƒ¼ãƒˆå‹•ç”»ï¼‰ã‚’ç”Ÿæˆã—ã¾ã™ã€‚3åˆ—ç›®ã¯**å…¬é–‹æ—¥(yyyymmdd)+å…ƒå‹•ç”»ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆãƒªãƒ³ã‚¯ä»˜ãï¼‰**ã§ã™ã€‚"
    )

    channel_url = st.text_input(
        "ãƒãƒ£ãƒ³ãƒãƒ«ã®URLï¼ˆ/channel/UCâ€¦ ã¾ãŸã¯ /@handleï¼‰",
        placeholder="https://www.youtube.com/@Google",
        key="shorts_channel_url",
    )
    max_items = st.slider(
        "å–å¾—ä»¶æ•°ï¼ˆä¸Šé™ï¼‰",
        min_value=5,
        max_value=200,
        value=50,
        step=5,
        key="shorts_max_items",
    )

    api_key_shorts = resolve_api_key(
        default_key=GLOBAL_API_KEY,
        input_state_key="shorts_api_key",
        expander_label="YouTube APIã‚­ãƒ¼ï¼ˆæ¨å¥¨ã€‚æœªè¨­å®šæ™‚ã¯ç°¡æ˜“ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã§è©¦è¡Œã€å…¬é–‹æ—¥ã¯å–å¾—ã§ãã¾ã›ã‚“ï¼‰",
    )

    run = st.button("å®Ÿè¡Œï¼ˆã‚·ãƒ§ãƒ¼ãƒˆå–å¾—â†’æ¨å®šâ†’CSVç”Ÿæˆï¼‰", key="shorts_run")

    if run:
        if not channel_url:
            st.error("ãƒãƒ£ãƒ³ãƒãƒ«URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        else:
            try:
                video_ids: List[str] = []
                titles: Dict[str, str] = {}
                ymd_map: Dict[str, Optional[str]] = {}

                if api_key_shorts:
                    ch_id = extract_channel_id_from_url(channel_url, api_key_shorts)
                    if not ch_id:
                        st.error("ãƒãƒ£ãƒ³ãƒãƒ«IDã‚’ç‰¹å®šã§ãã¾ã›ã‚“ã§ã—ãŸï¼ˆURLã‚’ç¢ºèªã™ã‚‹ã‹ã€@handle å½¢å¼ã®å ´åˆã¯APIã‚­ãƒ¼ãŒå¿…è¦ã§ã™ï¼‰ã€‚")
                        st.stop()
                    st.info(f"ãƒãƒ£ãƒ³ãƒãƒ«IDã‚’ç‰¹å®šã—ã¾ã—ãŸï¼š{ch_id}")
                    ids = list_channel_videos(ch_id, api_key_shorts, limit=max_items * 2)
                    metas = fetch_video_meta(ids, api_key_shorts)
                    # ã€Œ60ç§’ä»¥ä¸‹ã€ã‚’ã‚·ãƒ§ãƒ¼ãƒˆã¨ã¿ãªã™
                    shorts = [m for m in metas if m["seconds"] <= 61]
                    shorts = shorts[:max_items]
                    video_ids = [m["videoId"] for m in shorts]
                    titles = {m["videoId"]: m["title"] for m in shorts}
                    ymd_map = {m["videoId"]: m["yyyymmdd"] for m in shorts}
                else:
                    st.warning("APIã‚­ãƒ¼æœªè¨­å®šã®ãŸã‚ã€Webãƒšãƒ¼ã‚¸ã‹ã‚‰ã®ç°¡æ˜“æŠ½å‡ºã§è©¦è¡Œã—ã¾ã™ï¼ˆå…¬é–‹æ—¥ã¯å–å¾—ã§ãã¾ã›ã‚“ï¼‰ã€‚")
                    video_ids = scrape_shorts_ids_from_web(channel_url, limit=max_items)
                    # ã‚¿ã‚¤ãƒˆãƒ«ã¯ oEmbed ã§è£œå®Œï¼ˆå…¬é–‹æ—¥ã¯å–å¾—ä¸å¯ï¼‰
                    for vid in video_ids:
                        try:
                            j = requests.get(
                                "https://www.youtube.com/oembed",
                                params={"url": f"https://www.youtube.com/watch?v={vid}", "format": "json"},
                                timeout=6
                            ).json()
                            titles[vid] = (j.get("title") or "").strip()
                        except Exception:
                            titles[vid] = ""
                        ymd_map[vid] = None  # æ—¥ä»˜ã¯ç„¡ã—

                if not video_ids:
                    st.error("ã‚·ãƒ§ãƒ¼ãƒˆå‹•ç”»ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚URLã‚„æ¨©é™ã€å–å¾—ä»¶æ•°ã‚’è¦‹ç›´ã—ã¦ãã ã•ã„ã€‚")
                    st.stop()

                # æ¨å®šï¼†CSVä½œæˆï¼ˆ3åˆ—ï¼šã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆå, æ¥½æ›²å, ã‚·ãƒ§ãƒ¼ãƒˆå‹•ç”»ï¼‰
                rows = [["ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆå", "æ¥½æ›²å", "ã‚·ãƒ§ãƒ¼ãƒˆå‹•ç”»"]]
                preview = []
                for vid in video_ids:
                    title = titles.get(vid, "") or "ã‚·ãƒ§ãƒ¼ãƒˆå‹•ç”»"
                    artist, song = split_artist_song_from_title(title)
                    link = f"https://www.youtube.com/shorts/{vid}"

                    ymd = ymd_map.get(vid)
                    if ymd:
                        label = f"{ymd}{DATE_TITLE_SEPARATOR}{title}"
                    else:
                        label = title  # æ—¥ä»˜ãŒç„¡ã„å ´åˆã¯ã‚¿ã‚¤ãƒˆãƒ«ã®ã¿

                    hyperlink = make_excel_hyperlink(link, label)
                    rows.append([artist, song, hyperlink])
                    preview.append({
                        "videoId": vid,
                        "yyyymmdd": ymd or "",
                        "title": title,
                        "artist": artist,
                        "song": song,
                        "shorts_url": link
                    })

                st.success(f"å–å¾—ãƒ»æ¨å®šå®Œäº†ï¼š{len(preview)} ä»¶")
                st.dataframe(pd.DataFrame(preview), use_container_width=True)

                csv_text = to_csv(rows)
                st.download_button(
                    label="CSVã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                    data=csv_text.encode("utf-8-sig"),
                    file_name="shorts_songs.csv",
                    mime="text/csv"
                )

            except Exception as e:
                st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")
